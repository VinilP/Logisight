# Environment Configuration for Logistics Insight System
# Copy this file to .env and configure your preferred settings

# =============================================================================
# LLM Integration Configuration
# =============================================================================

# Preferred LLM Provider (optional)
# Options: "openai", "ollama", "rule_based"
# If not set, system will auto-detect and use best available option
# LLM_PROVIDER=openai

# =============================================================================
# OpenAI API Configuration (OPTION A)
# =============================================================================

# OpenAI API Key (required for OpenAI integration)
# Get your API key from: https://platform.openai.com/api-keys
# Estimated cost for demo usage: ~$5 should be sufficient
# OPENAI_API_KEY=your_openai_api_key_here

# OpenAI Model Selection (optional, defaults to gpt-3.5-turbo)
# Options: "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"
# OPENAI_MODEL=gpt-3.5-turbo

# =============================================================================
# Ollama Configuration (OPTION B)
# =============================================================================

# Ollama Model Name (optional, auto-detected if not set)
# Popular options: "llama2", "mistral", "codellama"
# Install models with: ollama pull llama2
# OLLAMA_MODEL=llama2

# Ollama Server URL (optional, defaults to local)
# OLLAMA_URL=http://localhost:11434

# =============================================================================
# System Configuration
# =============================================================================

# Enable debug logging (optional)
# DEBUG=false

# Maximum tokens for LLM responses (optional)
# MAX_TOKENS=800

# LLM request timeout in seconds (optional)
# LLM_TIMEOUT=30